"""
Report templates for consistent formatting across different report types.
"""

from typing import Dict, Any, List, Optional
from datetime import datetime
from pathlib import Path

from ..models.test_result import TestResult, TestStatus, ConfidenceLevel
from ..models.test_case import TestType, TestPriority


class ReportTemplates:
    """
    Centralized template system for generating consistent reports.
    
    Provides reusable templates and formatting utilities for different
    types of test coverage reports.
    """
    
    @staticmethod
    def get_executive_summary_template() -> str:
        """
        Get template for executive summary reports.
        
        Returns:
            Template string with placeholders
        """
        return """# Executive Summary - Test Coverage Report

**Generated:** {timestamp}
**Report Period:** {report_period}

## Key Metrics

| Metric | Value | Status |
|--------|-------|--------|
| Overall Success Rate | {success_rate}% | {success_status} |
| Total Tests Executed | {total_tests} | ‚ÑπÔ∏è |
| Critical Failures | {critical_failures} | {critical_status} |
| High Priority Issues | {high_priority_issues} | {priority_status} |

## Summary Statistics

- **Passed Tests:** {passed_tests} ({passed_percentage}%)
- **Failed Tests:** {failed_tests} ({failed_percentage}%)
- **Average Execution Time:** {avg_execution_time}s
- **Test Coverage:** {coverage_percentage}%

## üö® Critical Issues

{critical_issues_section}

## üìà Recommendations

{recommendations_section}

## üìã Next Steps

{next_steps_section}

---
*This report was automatically generated by the Feature Test Coverage System*
"""
    
    @staticmethod
    def get_detailed_report_template() -> str:
        """
        Get template for detailed technical reports.
        
        Returns:
            Template string with placeholders
        """
        return """# Detailed Test Coverage Report

**Generated:** {timestamp}
**System Version:** {system_version}
**Test Environment:** {test_environment}

## üìã Test Execution Summary

{execution_summary}

## üîç Feature Analysis

{feature_analysis}

## Test Type Breakdown

{test_type_breakdown}

## ‚ö†Ô∏è Failure Analysis

{failure_analysis}

## Performance Metrics

{performance_metrics}

## Trend Analysis

{trend_analysis}

## üìù Detailed Results

{detailed_results}

## üõ†Ô∏è Technical Recommendations

{technical_recommendations}

---
*Report generated by {generator_name} v{generator_version}*
"""
    
    @staticmethod
    def get_feature_report_template() -> str:
        """
        Get template for individual feature reports.
        
        Returns:
            Template string with placeholders
        """
        return """# Feature Report: {feature_name}

**Generated:** {timestamp}
**Feature Category:** {feature_category}
**Priority Level:** {priority_level}

## Feature Overview

{feature_overview}

## Test Results Summary

| Test Type | Total | Passed | Failed | Success Rate |
|-----------|-------|--------|--------|--------------|
{test_type_table}

## Passed Tests

{passed_tests_section}

## Failed Tests

{failed_tests_section}

## üìà Performance Analysis

{performance_analysis}

## üîç Economic Hypothesis Validation

{hypothesis_validation}

## ‚ö†Ô∏è Risk Assessment

{risk_assessment}

## üí° Recommendations

{feature_recommendations}

---
*Feature analysis completed at {completion_time}*
"""
    
    @staticmethod
    def get_coverage_matrix_template() -> str:
        """
        Get template for test coverage matrix.
        
        Returns:
            Template string with placeholders
        """
        return """# Test Coverage Matrix

**Generated:** {timestamp}
**Coverage Scope:** {coverage_scope}

## Coverage Overview

{coverage_overview}

## Feature Coverage Matrix

{coverage_matrix}

## üìã Test Type Coverage

{test_type_coverage}

## üî¥ Coverage Gaps

{coverage_gaps}

## üìà Coverage Trends

{coverage_trends}

## Coverage Goals

{coverage_goals}

---
*Coverage analysis performed by automated testing system*
"""
    
    @staticmethod
    def format_executive_summary(
        results: List[TestResult],
        report_period: str = "Current",
        system_version: str = "1.0.0"
    ) -> str:
        """
        Format an executive summary using the template.
        
        Args:
            results: List of test results
            report_period: Period covered by the report
            system_version: Version of the system being tested
            
        Returns:
            Formatted executive summary
        """
        if not results:
            return "No test results available for executive summary."
        
        # Calculate metrics
        total_tests = len(results)
        passed_tests = sum(1 for r in results if r.passed)
        failed_tests = total_tests - passed_tests
        success_rate = (passed_tests / total_tests) * 100 if total_tests > 0 else 0
        
        # Calculate percentages
        passed_percentage = (passed_tests / total_tests) * 100 if total_tests > 0 else 0
        failed_percentage = (failed_tests / total_tests) * 100 if total_tests > 0 else 0
        
        # Calculate execution time
        execution_times = [r.execution_time for r in results if r.execution_time > 0]
        avg_execution_time = sum(execution_times) / len(execution_times) if execution_times else 0
        
        # Count critical issues
        critical_failures = sum(
            1 for r in results 
            if not r.passed and r.test_case.priority == TestPriority.CRITICAL
        )
        
        high_priority_issues = sum(
            1 for r in results 
            if not r.passed and r.test_case.priority == TestPriority.HIGH
        )
        
        # Determine status indicators
        success_status = "‚úÖ" if success_rate >= 95 else "‚ö†Ô∏è" if success_rate >= 80 else "‚ùå"
        critical_status = "‚úÖ" if critical_failures == 0 else "‚ùå"
        priority_status = "‚úÖ" if high_priority_issues == 0 else "‚ö†Ô∏è" if high_priority_issues <= 2 else "‚ùå"
        
        # Generate critical issues section
        critical_issues_section = ReportTemplates._format_critical_issues(results)
        
        # Generate recommendations
        recommendations_section = ReportTemplates._format_recommendations(results)
        
        # Generate next steps
        next_steps_section = ReportTemplates._format_next_steps(results)
        
        # Calculate coverage (simplified)
        coverage_percentage = success_rate  # Simplified coverage calculation
        
        template = ReportTemplates.get_executive_summary_template()
        
        return template.format(
            timestamp=datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            report_period=report_period,
            success_rate=f"{success_rate:.1f}",
            success_status=success_status,
            total_tests=total_tests,
            critical_failures=critical_failures,
            critical_status=critical_status,
            high_priority_issues=high_priority_issues,
            priority_status=priority_status,
            passed_tests=passed_tests,
            passed_percentage=f"{passed_percentage:.1f}",
            failed_tests=failed_tests,
            failed_percentage=f"{failed_percentage:.1f}",
            avg_execution_time=f"{avg_execution_time:.2f}",
            coverage_percentage=f"{coverage_percentage:.1f}",
            critical_issues_section=critical_issues_section,
            recommendations_section=recommendations_section,
            next_steps_section=next_steps_section
        )
    
    @staticmethod
    def format_feature_report(
        feature_name: str,
        results: List[TestResult],
        feature_category: str = "Unknown",
        priority_level: str = "Medium"
    ) -> str:
        """
        Format a feature-specific report using the template.
        
        Args:
            feature_name: Name of the feature
            results: List of test results for this feature
            feature_category: Category of the feature
            priority_level: Priority level of the feature
            
        Returns:
            Formatted feature report
        """
        # Filter results for this feature
        feature_results = [r for r in results if r.test_case.feature_name == feature_name]
        
        if not feature_results:
            return f"No test results found for feature: {feature_name}"
        
        # Generate sections
        feature_overview = ReportTemplates._format_feature_overview(feature_results)
        test_type_table = ReportTemplates._format_test_type_table(feature_results)
        passed_tests_section = ReportTemplates._format_passed_tests(feature_results)
        failed_tests_section = ReportTemplates._format_failed_tests(feature_results)
        performance_analysis = ReportTemplates._format_performance_analysis(feature_results)
        hypothesis_validation = ReportTemplates._format_hypothesis_validation(feature_results)
        risk_assessment = ReportTemplates._format_risk_assessment(feature_results)
        feature_recommendations = ReportTemplates._format_feature_recommendations(feature_results)
        
        template = ReportTemplates.get_feature_report_template()
        
        return template.format(
            feature_name=feature_name,
            timestamp=datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            feature_category=feature_category,
            priority_level=priority_level,
            feature_overview=feature_overview,
            test_type_table=test_type_table,
            passed_tests_section=passed_tests_section,
            failed_tests_section=failed_tests_section,
            performance_analysis=performance_analysis,
            hypothesis_validation=hypothesis_validation,
            risk_assessment=risk_assessment,
            feature_recommendations=feature_recommendations,
            completion_time=datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        )
    
    @staticmethod
    def _format_critical_issues(results: List[TestResult]) -> str:
        """Format critical issues section."""
        critical_failures = [
            r for r in results 
            if not r.passed and r.test_case.priority == TestPriority.CRITICAL
        ]
        
        if not critical_failures:
            return "**No critical issues found.** All critical tests are passing."
        
        issues = ["**Critical issues requiring immediate attention:**\n"]
        
        for i, failure in enumerate(critical_failures[:5], 1):  # Top 5 critical issues
            issues.append(f"{i}. **{failure.test_case.feature_name}** - {failure.test_case.test_type.value}")
            issues.append(f"   - Error: {failure.error_message or 'No error message'}")
            issues.append(f"   - Impact: {failure.analysis or 'Analysis pending'}")
            if failure.recommendations:
                issues.append(f"   - Action: {failure.recommendations[0]}")
            issues.append("")
        
        return "\n".join(issues)
    
    @staticmethod
    def _format_recommendations(results: List[TestResult]) -> str:
        """Format recommendations section."""
        failed_results = [r for r in results if not r.passed]
        
        if not failed_results:
            return "**No immediate actions required.** All tests are passing successfully."
        
        # Collect unique recommendations
        all_recommendations = []
        for result in failed_results:
            all_recommendations.extend(result.recommendations)
        
        unique_recommendations = list(set(all_recommendations))[:10]  # Top 10
        
        if not unique_recommendations:
            return "‚ö†Ô∏è **Manual review required.** Failed tests need analysis and recommendations."
        
        recommendations = ["**Key recommendations for improvement:**\n"]
        for i, rec in enumerate(unique_recommendations, 1):
            recommendations.append(f"{i}. {rec}")
        
        return "\n".join(recommendations)
    
    @staticmethod
    def _format_next_steps(results: List[TestResult]) -> str:
        """Format next steps section."""
        critical_failures = sum(
            1 for r in results 
            if not r.passed and r.test_case.priority == TestPriority.CRITICAL
        )
        
        high_priority_failures = sum(
            1 for r in results 
            if not r.passed and r.test_case.priority == TestPriority.HIGH
        )
        
        steps = []
        
        if critical_failures > 0:
            steps.append("1. **IMMEDIATE:** Address all critical test failures before any deployment")
            steps.append("2. **URGENT:** Conduct root cause analysis for critical issues")
        
        if high_priority_failures > 0:
            steps.append("3. **HIGH:** Review and fix high priority test failures")
            steps.append("4. **MEDIUM:** Update test documentation and procedures")
        
        if not steps:
            steps.append("1. **MAINTAIN:** Continue monitoring test results for any degradation")
            steps.append("2. **IMPROVE:** Consider adding additional test coverage")
        
        steps.append("5. **ONGOING:** Schedule regular test coverage reviews")
        
        return "\n".join(steps)
    
    @staticmethod
    def _format_feature_overview(results: List[TestResult]) -> str:
        """Format feature overview section."""
        total_tests = len(results)
        passed_tests = sum(1 for r in results if r.passed)
        success_rate = (passed_tests / total_tests) * 100 if total_tests > 0 else 0
        
        # Calculate average confidence
        avg_confidence = sum(r.confidence_score for r in results) / len(results) if results else 0
        
        # Calculate execution time
        execution_times = [r.execution_time for r in results if r.execution_time > 0]
        avg_execution_time = sum(execution_times) / len(execution_times) if execution_times else 0
        
        overview = [
            f"- **Total Tests:** {total_tests}",
            f"- **Success Rate:** {success_rate:.1f}%",
            f"- **Average Confidence:** {avg_confidence:.2f}",
            f"- **Average Execution Time:** {avg_execution_time:.2f}s"
        ]
        
        return "\n".join(overview)
    
    @staticmethod
    def _format_test_type_table(results: List[TestResult]) -> str:
        """Format test type table."""
        # Group by test type
        test_type_stats = {}
        for test_type in TestType:
            type_results = [r for r in results if r.test_case.test_type == test_type]
            if type_results:
                passed = sum(1 for r in type_results if r.passed)
                failed = len(type_results) - passed
                success_rate = (passed / len(type_results)) * 100
                
                test_type_stats[test_type.value] = {
                    'total': len(type_results),
                    'passed': passed,
                    'failed': failed,
                    'success_rate': success_rate
                }
        
        if not test_type_stats:
            return "| No test types found | - | - | - | - |"
        
        table_rows = []
        for test_type, stats in test_type_stats.items():
            table_rows.append(
                f"| {test_type.replace('_', ' ').title()} | {stats['total']} | "
                f"{stats['passed']} | {stats['failed']} | {stats['success_rate']:.1f}% |"
            )
        
        return "\n".join(table_rows)
    
    @staticmethod
    def _format_passed_tests(results: List[TestResult]) -> str:
        """Format passed tests section."""
        passed_tests = [r for r in results if r.passed]
        
        if not passed_tests:
            return "No tests passed for this feature."
        
        sections = []
        for test in passed_tests:
            sections.append(f"### {test.test_case.test_type.value.replace('_', ' ').title()}")
            sections.append(f"- **Confidence:** {test.confidence.value} ({test.confidence_score:.2f})")
            sections.append(f"- **Execution Time:** {test.execution_time:.2f}s")
            if test.analysis:
                sections.append(f"- **Analysis:** {test.analysis}")
            sections.append("")
        
        return "\n".join(sections)
    
    @staticmethod
    def _format_failed_tests(results: List[TestResult]) -> str:
        """Format failed tests section."""
        failed_tests = [r for r in results if not r.passed]
        
        if not failed_tests:
            return "All tests passed for this feature."
        
        sections = []
        for test in failed_tests:
            sections.append(f"### {test.test_case.test_type.value.replace('_', ' ').title()}")
            sections.append(f"- **Status:** {test.status.value}")
            sections.append(f"- **Priority:** {test.test_case.priority.value}")
            if test.error_message:
                sections.append(f"- **Error:** {test.error_message}")
            if test.analysis:
                sections.append(f"- **Analysis:** {test.analysis}")
            if test.recommendations:
                sections.append("- **Recommendations:**")
                for rec in test.recommendations:
                    sections.append(f"  - {rec}")
            sections.append("")
        
        return "\n".join(sections)
    
    @staticmethod
    def _format_performance_analysis(results: List[TestResult]) -> str:
        """Format performance analysis section."""
        # Collect performance metrics
        all_metrics = {}
        for result in results:
            for metric, value in result.performance_metrics.items():
                if metric not in all_metrics:
                    all_metrics[metric] = []
                all_metrics[metric].append(value)
        
        if not all_metrics:
            return "No performance metrics available for this feature."
        
        analysis = []
        for metric, values in all_metrics.items():
            avg_value = sum(values) / len(values)
            min_value = min(values)
            max_value = max(values)
            
            analysis.append(f"### {metric.replace('_', ' ').title()}")
            analysis.append(f"- **Average:** {avg_value:.3f}")
            analysis.append(f"- **Range:** {min_value:.3f} - {max_value:.3f}")
            analysis.append("")
        
        return "\n".join(analysis)
    
    @staticmethod
    def _format_hypothesis_validation(results: List[TestResult]) -> str:
        """Format economic hypothesis validation section."""
        hypothesis_tests = [
            r for r in results 
            if r.test_case.test_type == TestType.ECONOMIC_HYPOTHESIS
        ]
        
        if not hypothesis_tests:
            return "No economic hypothesis tests found for this feature."
        
        validation = []
        for test in hypothesis_tests:
            status_icon = "‚úÖ" if test.passed else "‚ùå"
            validation.append(f"{status_icon} **Hypothesis Test:** {test.test_case.description or 'Economic behavior validation'}")
            
            if test.passed:
                validation.append(f"- **Result:** Hypothesis validated with {test.confidence.value} confidence")
            else:
                validation.append(f"- **Result:** Hypothesis validation failed")
                validation.append(f"- **Issue:** {test.analysis or 'Analysis pending'}")
            
            validation.append("")
        
        return "\n".join(validation)
    
    @staticmethod
    def _format_risk_assessment(results: List[TestResult]) -> str:
        """Format risk assessment section."""
        risk_levels = {'high': 0, 'medium': 0, 'low': 0}
        
        for result in results:
            risk_assessment = result.get_risk_assessment()
            risk_level = risk_assessment['risk_level']
            risk_levels[risk_level] += 1
        
        assessment = [
            f"- **High Risk Issues:** {risk_levels['high']}",
            f"- **Medium Risk Issues:** {risk_levels['medium']}",
            f"- **Low Risk Issues:** {risk_levels['low']}"
        ]
        
        # Overall risk level
        if risk_levels['high'] > 0:
            overall_risk = "üî¥ HIGH"
            assessment.append("\n**Overall Risk Level:** HIGH - Immediate attention required")
        elif risk_levels['medium'] > 2:
            overall_risk = "üü° MEDIUM"
            assessment.append("\n**Overall Risk Level:** MEDIUM - Monitor closely")
        else:
            overall_risk = "üü¢ LOW"
            assessment.append("\n**Overall Risk Level:** LOW - Acceptable risk")
        
        return "\n".join(assessment)
    
    @staticmethod
    def _format_feature_recommendations(results: List[TestResult]) -> str:
        """Format feature-specific recommendations."""
        failed_results = [r for r in results if not r.passed]
        
        if not failed_results:
            return "**No specific recommendations.** Feature is performing well."
        
        # Collect all recommendations
        all_recommendations = []
        for result in failed_results:
            all_recommendations.extend(result.recommendations)
        
        unique_recommendations = list(set(all_recommendations))
        
        if not unique_recommendations:
            return "‚ö†Ô∏è **Manual analysis required.** Failed tests need detailed review."
        
        recommendations = []
        for i, rec in enumerate(unique_recommendations, 1):
            recommendations.append(f"{i}. {rec}")
        
        return "\n".join(recommendations)
    
    @staticmethod
    def get_html_template_styles() -> str:
        """
        Get CSS styles for HTML report templates.
        
        Returns:
            CSS styles string
        """
        return """
        <style>
            body {
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
                line-height: 1.6;
                color: #333;
                max-width: 1200px;
                margin: 0 auto;
                padding: 20px;
                background-color: #f8f9fa;
            }
            
            .report-container {
                background: white;
                padding: 40px;
                border-radius: 8px;
                box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            }
            
            h1 {
                color: #2c3e50;
                border-bottom: 3px solid #3498db;
                padding-bottom: 10px;
                margin-bottom: 30px;
            }
            
            h2 {
                color: #34495e;
                border-bottom: 1px solid #ecf0f1;
                padding-bottom: 5px;
                margin-top: 30px;
                margin-bottom: 20px;
            }
            
            h3 {
                color: #7f8c8d;
                margin-top: 25px;
                margin-bottom: 15px;
            }
            
            table {
                width: 100%;
                border-collapse: collapse;
                margin: 20px 0;
                background: white;
            }
            
            th, td {
                border: 1px solid #ddd;
                padding: 12px;
                text-align: left;
            }
            
            th {
                background-color: #f2f2f2;
                font-weight: bold;
                color: #2c3e50;
            }
            
            tr:nth-child(even) {
                background-color: #f9f9f9;
            }
            
            .success {
                color: #27ae60;
                font-weight: bold;
            }
            
            .warning {
                color: #f39c12;
                font-weight: bold;
            }
            
            .error {
                color: #e74c3c;
                font-weight: bold;
            }
            
            .metric-card {
                background: #ecf0f1;
                padding: 20px;
                border-radius: 5px;
                margin: 15px 0;
                border-left: 4px solid #3498db;
            }
            
            .critical-issue {
                background: #fdf2f2;
                border-left: 4px solid #e74c3c;
                padding: 15px;
                margin: 10px 0;
                border-radius: 4px;
            }
            
            .recommendation {
                background: #f0f9ff;
                border-left: 4px solid #3b82f6;
                padding: 15px;
                margin: 10px 0;
                border-radius: 4px;
            }
            
            .timestamp {
                color: #7f8c8d;
                font-size: 0.9em;
                font-style: italic;
            }
            
            code {
                background-color: #f8f9fa;
                padding: 2px 4px;
                border-radius: 3px;
                font-family: 'Monaco', 'Consolas', monospace;
            }
            
            pre {
                background-color: #f8f9fa;
                padding: 15px;
                border-radius: 5px;
                overflow-x: auto;
                border: 1px solid #e9ecef;
            }
            
            .footer {
                margin-top: 40px;
                padding-top: 20px;
                border-top: 1px solid #ecf0f1;
                text-align: center;
                color: #7f8c8d;
                font-size: 0.9em;
            }
        </style>
        """
    
    @staticmethod
    def wrap_in_html_template(content: str, title: str = "Test Coverage Report") -> str:
        """
        Wrap content in a complete HTML template.
        
        Args:
            content: Report content (HTML or markdown)
            title: Page title
            
        Returns:
            Complete HTML document
        """
        return f"""<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{title}</title>
    {ReportTemplates.get_html_template_styles()}
</head>
<body>
    <div class="report-container">
        {content}
        <div class="footer">
            <p>Generated by Feature Test Coverage System</p>
            <p class="timestamp">Report created: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        </div>
    </div>
</body>
</html>"""
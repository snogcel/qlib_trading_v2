import qlib
from qlib.constant import REG_US, REG_CN 
from qlib.utils import init_instance_by_config, flatten_dict
from sklearn.metrics import mean_squared_error as MSE
from sklearn.metrics import r2_score, accuracy_score
from qlib.data.dataset.handler import DataHandlerLP

# Fixed imports using new src structure
from src.data.nested_data_loader import CustomNestedDataLoader
from src.models.signal_environment import SignalEnv
from src.models.multi_quantile import QuantileLGBModel, MultiQuantileModel
from src.data.crypto_loader import crypto_dataloader_optimized as crypto_dataloader
from src.data.gdelt_loader import gdelt_dataloader_optimized as gdelt_dataloader
from src.features.position_sizing import AdvancedPositionSizer


# TierLoggingCallback moved to RL execution folder
from src.rl_execution.custom_tier_logging import TierLoggingCallback

import optuna
import lightgbm as lgbm

from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv

from qlib.workflow import R
from qlib.workflow.record_temp import SignalRecord, PortAnaRecord, SigAnaRecord

from qlib.data.dataset.processor import Processor
from qlib.utils import get_callable_kwargs
from qlib.data.dataset import processor as processor_module
from inspect import getfullargspec

from sklearn.model_selection import TimeSeriesSplit


VOL_RAW_THRESHOLDS - static variable

DECILE_THRESHOLDS - static thresholds


def quantile_loss (function)

def signal_classification

def get_vol_raw_decile(vol_raw_value)

def get_decile_rank(value, thresholds)

def q50_regime_aware_signals(df, transaction_cost_bps, base_info_ratio)

def identify_market_regumes(df)

def kelly_sizing(row)

def prob_up)piecewise(row)

def check_transform_proc ** QLIB function**

def adaptive_entropy_coef(vol_scaled,base,min_coef, max_coef)



class EntropyAwarePPO(PPO) -- object oriented fundamentals.... this is more c + data structures + javascript + jquery + nodejs + bitcore-lib meets php



_________


CORE_LGBM_PARAMS - static variable
GENERIC_LGBM_PARAMS - static variable
multi_quantile_params - static variable


CORE_LGBM_PARAMS = {
        "objective": "quantile",
        "metric": ["l1", "l2"], # , "l2", "l1" # "rmse"
        "boosting_type": "gbdt",
        "device": "cpu",
        "verbose": 1, # set to verbose for more logs (this is outrageously complex lol)
        "random_state": 141551, # https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/
        "early_stopping_rounds": 500,
        "num_boost_round": 2250,         # Let early stopping decide
        "seed": SEED
    }

    GENERIC_LGBM_PARAMS = {       
        # Conservative learning settings for feature exploration
        "learning_rate": 0.05,           # Moderate learning rate
        # "num_leaves": 64,                # Balanced complexity
        # "max_depth": 8,                  # Reasonable depth for GDELT features
        
        # Regularization (moderate to prevent overfitting)
        "lambda_l1": 0.1,
        "lambda_l2": 0.1,
        
        # "min_data_in_leaf": 20, # remove constraint for tuning
        
        "feature_fraction": 0.8,         # Use 80% of features per tree
        "bagging_fraction": 0.8,         # Use 80% of data per iteration
        "bagging_freq": 5,
    }

    multi_quantile_params = {
        # 0.1: {'learning_rate': 0.060555113429817814, 'colsample_bytree': 0.7214813020361056, 'subsample': 0.7849919729082881, 'lambda_l1': 8.722794281828277e-05, 'lambda_l2': 3.220667556916701e-05, 'max_depth': 10, 'num_leaves': 224, **GENERIC_LGBM_PARAMS},
        # 0.5: {'learning_rate': 0.02753370821225369, 'max_depth': -1, 'lambda_l1': 0.1, 'lambda_l2': 0.1, **GENERIC_LGBM_PARAMS},
        # 0.9: {'learning_rate': 0.09355380738420341, 'max_depth': 10, 'num_leaves': 249, 'lambda_l1': 0.1, 'lambda_l2': 0.1, **GENERIC_LGBM_PARAMS}

        # 0.1: {**CORE_LGBM_PARAMS},
        # 0.5: {**CORE_LGBM_PARAMS},                
        # 0.9: {**CORE_LGBM_PARAMS} 

        0.1: {'max_depth': 25, **CORE_LGBM_PARAMS},
        0.5: {'max_depth': 25, **CORE_LGBM_PARAMS},                
        0.9: {'max_depth': 25, **CORE_LGBM_PARAMS}

    }





__________________



def cross_validation_fcn(df_train, model, early_stopping_flag) # called by objective function (clarified below)




params = {
            "objective": "quantile",
            "metric": ["l2", "l1"],
            "boosting_type": "gbdt",
            "device": "cpu",
            "verbose": 1,
            
            "alpha": 0.1, # this controls which percentile the predictive model is targeting - in this case, Q10                     

            # Regularization (moderate to prevent overfitting)
            #"lambda_l1": 0.1,
            #"lambda_l2": 0.1,
            #"min_data_in_leaf": 20,
            #"feature_fraction": 0.8,         # Use 80% of features per tree
            #"bagging_fraction": 0.8,         # Use 80% of data per iteration
            #"bagging_freq": 5,
            
            #"colsample_bytree": trial.suggest_float("colsample_bytree", 0.5, 1.0),
            #"subsample": trial.suggest_float("subsample", 0.5, 1.0),

            # "learning_rate": 0.05628507997416036,
            # "max_depth": 8,            
            # "num_leaves": 163,
            # "lambda_l1": 4.511969685016852, 
            # "lambda_l2": 0.0006936273081692159,
            
            # "min_data_in_leaf": 20, # remove constraint to prevent overfitting

            # "feature_fraction": 0.8,         # Use 80% of features per tree
            # "bagging_fraction": 0.8,         # Use 80% of data per iteration
            # "bagging_freq": 5,
            
            "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.1, log=True),
            "colsample_bytree": trial.suggest_float("colsample_bytree", 0.7, 1.0),
            "subsample": trial.suggest_float("subsample", 0.7, 1.0),
            "lambda_l1": trial.suggest_loguniform("lambda_l1", 1e-8, 10.0),
            "lambda_l2": trial.suggest_loguniform("lambda_l2", 1e-8, 10.0),
            "max_depth": trial.suggest_int("max_depth", 4, 10), # increase max depth to 50
            "num_leaves": trial.suggest_int("num_leaves", 20, 512),    

            "min_data_in_leaf": trial.suggest_int("min_data_in_leaf", 2, 512), # use optuna to find the optimilar data points within each point of decision tree (LGBM) - doesn't apply to LGBM model AFAIK          
            
            # "max_depth": trial.suggest_int("max_depth", 4, 10),
           

            # Early stopping
            "early_stopping_rounds": 1000, # increase from 100 to 1000 training rounds
            "num_boost_round": 1000,         # Let early stopping decide

            # Set seed for reproducibility
            "seed": SEED
        }

        # create the LightGBM regressor with the optimized parameters
        model = lgbm.LGBMRegressor(**params)

        # perform cross-validation using the optimized LightGBM regressor
        lgbm_model, mean_score = cross_validation_fcn(df_train, model, early_stopping_flag=True)






























_____________________




entry:

if __name__ == '__main__':

# define the objective function for Optuna optimization
def objective(trial)


# this variable controls what is being tuned:

params = {
            "objective": "quantile",
            "metric": ["l2", "l1"],
            "boosting_type": "gbdt",
            "device": "cpu",
            "verbose": 1,
            
            "alpha": 0.1, # this controls which percentile the predictive model is targeting - in this case, Q10                     

            # Regularization (moderate to prevent overfitting)
            #"lambda_l1": 0.1,
            #"lambda_l2": 0.1,
            #"min_data_in_leaf": 20,
            #"feature_fraction": 0.8,         # Use 80% of features per tree
            #"bagging_fraction": 0.8,         # Use 80% of data per iteration
            #"bagging_freq": 5,
            
            #"colsample_bytree": trial.suggest_float("colsample_bytree", 0.5, 1.0),
            #"subsample": trial.suggest_float("subsample", 0.5, 1.0),

            # "learning_rate": 0.05628507997416036,
            # "max_depth": 8,            
            # "num_leaves": 163,
            # "lambda_l1": 4.511969685016852, 
            # "lambda_l2": 0.0006936273081692159,
            
            # "min_data_in_leaf": 20, # remove constraint to prevent overfitting

            # "feature_fraction": 0.8,         # Use 80% of features per tree
            # "bagging_fraction": 0.8,         # Use 80% of data per iteration
            # "bagging_freq": 5,
            
            "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.1, log=True),
            "colsample_bytree": trial.suggest_float("colsample_bytree", 0.7, 1.0),
            "subsample": trial.suggest_float("subsample", 0.7, 1.0),
            "lambda_l1": trial.suggest_loguniform("lambda_l1", 1e-8, 10.0),
            "lambda_l2": trial.suggest_loguniform("lambda_l2", 1e-8, 10.0),
            "max_depth": trial.suggest_int("max_depth", 4, 10), # increase max depth to 50
            "num_leaves": trial.suggest_int("num_leaves", 20, 512),    

            "min_data_in_leaf": trial.suggest_int("min_data_in_leaf", 2, 512), # use optuna to find the optimilar data points within each point of decision tree (LGBM) - doesn't apply to LGBM model AFAIK          
            
            # "max_depth": trial.suggest_int("max_depth", 4, 10),
           

            # Early stopping
            "early_stopping_rounds": 1000, # increase from 100 to 1000 training rounds
            "num_boost_round": 1000,         # Let early stopping decide

            # Set seed for reproducibility
            "seed": SEED
        }

	# create the LightGBM regressor with the optimized parameters
        model = lgbm.LGBMRegressor(**params)

        # perform cross-validation using the optimized LightGBM regressor
        lgbm_model, mean_score = cross_validation_fcn(df_train, model, early_stopping_flag=True)

        # retrieve the best iteration of the model and store it as a user attribute in the trial object
        best_iteration = lgbm_model.best_iteration_
        trial.set_user_attr('best_iteration', best_iteration)
            
        return mean_score


... complicated stuff ...

# then outputs

correlation_matrix = df_all.corr()        
    correlation_matrix.to_csv("./temp/correlation_matrix.csv")
        
    # Drop redundant columns if needed (updated for optimized loader)
    columns_to_drop = []

    if columns_to_drop:
        df_all.drop(columns_to_drop, axis=1, inplace=True)
        print(f"Dropped columns: {columns_to_drop}")
    
    df_all.to_csv("./temp/df_all_macro_analysis.csv")

    df_cleaned = df_all.dropna(subset=["vol_risk","vol_scaled","vol_raw_momentum","signal_thresh_adaptive","signal_tanh","enhanced_info_ratio"])

    df_cleaned.to_pickle("./data3/macro_features.pkl") # pickled features used in "train_meta_wrapper.py" process





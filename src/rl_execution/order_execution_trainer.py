import pandas as pd
import numpy as np
import os
import qlib
from qlib.constant import REG_US
from qlib.utils import init_instance_by_config
from qlib.workflow import R
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv

from src.models.signal_environment import SignalEnv
from qlib_custom.custom_tier_logging import TierLoggingCallback
from rl_order_execution.order_execution_env import create_env

# --- Configuration ---
provider_uri = "c:/Projects/qlib_trading_v2/qlib_data/CRYPTODATA"
qlib.init(provider_uri=provider_uri, region=REG_US)

SEED = 42
MARKET = "all"
BENCHMARK = "BTCUSDT"
EXP_NAME = "order_execution_exp_1"
FREQ = "day"
TOTAL_TIMESTEPS = 184_000

# --- PPO Hyperparameters ---
PPO_PARAMS = {
    "learning_rate": 3e-4,
    "clip_range": 0.2,
    "ent_coef": 0.005,
    "gae_lambda": 0.95,
    "vf_coef": 0.5,
}

# --- Main Execution ---
if __name__ == '__main__':
    # --- 1. Load and Prepare Data ---
    # This assumes df_cleaned.csv is already generated by ppo_sweep_runner.py
    # In a production setup, this data preparation would be a separate, explicit step.
    try:
        df_cleaned = pd.read_csv("df_cleaned.csv", index_col=0, parse_dates=True)
    except FileNotFoundError:
        print("Error: df_cleaned.csv not found. Please run ppo_sweep_runner.py first to generate the feature data.")
        exit()

    df_train = df_cleaned.loc["2018-02-01":"2023-12-31"]
    df_val = df_cleaned.loc["2024-01-01":"2024-09-30"]

    # --- 2. Define Environment Configuration ---
    # The environment will now be an order execution environment
    # that uses our FeatureAwareExchange to get observations.
    env_config = {
        "order": {
            "start_time": "2018-02-01",
            "end_time": "2023-12-31",
            "amount": 1.0, # Example: execute an order of 1 BTC
            "direction": "buy"
        },
        "exchange": {
            "feature_df_path": "df_cleaned.csv",
            "codes": [BENCHMARK],
            "freq": FREQ,
        }
    }

    # --- 3. Create and Set Up Environment ---
    env_train = create_env(env_config)
    vec_env = DummyVecEnv([lambda: env_train])
    vec_env.seed(SEED)

    # --- 4. Set Up Agent ---
    # Note: The EntropyAwarePPO is not used here as the volatility-based
    # entropy scheduling was part of the previous signal-based environment.
    # This can be re-integrated if desired.
    agent = PPO(
        policy="MlpPolicy",
        env=vec_env,
        seed=SEED,
        verbose=1,
        tensorboard_log=f"./logs_v2/{EXP_NAME}",
        **PPO_PARAMS
    )

    # --- 5. Train Agent ---
    print(f"ðŸš€ Launching Training: {EXP_NAME}")
    agent.learn(total_timesteps=TOTAL_TIMESTEPS)
    agent.save(f"./models/{EXP_NAME}")
    print(f"âœ… Training Complete. Model saved to ./models/{EXP_NAME}")

    # --- 6. Evaluation (Simplified) ---
    # A full backtest would be a separate script, similar to the
    # rl_order_execution/workflow.py example.
    print("ðŸ§ª Running Simplified Evaluation...")
    obs = env_train.reset()
    done = False
    total_reward = 0
    while not done:
        action, _ = agent.predict(obs, deterministic=True)
        obs, reward, done, info = env_train.step(action)
        total_reward += reward
    print(f"Simplified Evaluation Complete. Total Reward: {total_reward:.4f}")